# Depression_Analysis
Depression is the leading cause of suicide worldwide and many a times this mental condition gets reflected through social media post. The approach which was planned for solving the problem, after reading papers on the work which have been done on this in the past, was that the tweets would be segmented into two categories- depressed and random through building a deep learning model and classification models. These tweets would be analyzed for specific linguistic markers, time and frequency of posting of tweets which could then be used for identifying the deteriorating mental condition in case of depression. The social media platform chosen for collecting data was twitter. The idea behind choosing twitter over other social media platforms was that tweets are random and does not necessarily have personal connections, so it was easy to collect a lot of data from it. Currently working on this project wherein twitter data of ten years from 01.06.2010 to 15.08.2020 were collected. So at first for depressed tweets dataset specific set of keywords like "depression", "depressed", "anxiety", "overwhelmed", "exhausted", "distressed", "anxious", "tired", "low", "discouraged", "desperate", "demotivated", "insomnia", "cry", "nervous", "worried", "lonely", "sad", "empty", "hopeless" , "antidepressant", etc. were used. These keywords were taken from a paper Leis et al wherein it was stated that these words were mostly used by people suffering from depression in clinical settings. For random tweets dataset the keywords used were "great", "life", "feeling", "win", "live", "best", "cool", "hope", "know", "lol", "music", "school", "college", "fun", "today", "want", "time", "back", "call", "look", "first", "please", "everyone", "amazing", "summer", "rain", "winter", "wait", "cause", "coming", "even", "odd", "maybe", "head", "big", "small", "soon", "later", "run", "lot", "show", "least", "most", "yeah", "way", "hi", "hello", etc. After extracting tweets using keywords, specific twitter users were selected based on their frequency of tweeting using these keywords. Then top tweets were collected of these users for ten years and grouped into two datasets- depressed user dataset and random user dataset. But the tweet classification were not appropiate. There were many tweets in the depressed user dataset which looked normal through intuition. For this reason the datasets were merged into one and finally machine learning models were build to classify these tweets.

For data collection it was specifically found that the data extraction process using twitter API was slow. Maximum allowed count of tweets were 100 and that historical tweets were not accessible. Therefore used an alternate tool to get huge amount of historical tweets, which was the modified GetOldTweets-python tool originally developed by Jefferson Henrique and later modified by Victor E. Irekponor. Github link for the same- https://github.com/marquisvictor/Optimized-Modified-GetOldTweets3-OMGOT.git. The code uptil these steps can be found in Twitter_Data_Extraction1(GetOldTweets).ipynb and Twitter_Data_Extraction2(Twitter_API).ipynb files.

After data collection, preprocessing of the text data was the next step. These steps were done using the libraries associated with nltk package in python. This code can be found in Depression analysis from tweets (Data preprocessing).ipynb file. Used bag of words,TF-IDF and word2vec approach but finally proceeded with word2vec for word embedding. Now the major issue with the dataset was that for tweet classification into depressed and random tweets, trusted labels were not available for it. So started searching for a labelled dataset for training the model. The dataset which was then used can be found in this github link https://github.com/viritaromero/Detecting-Depression-in-Tweets.git. This dataset was trained using a LSTM model and classification models- Random Forest Classifier, Multinomial NB and Passive Aggressive Classifier. Repeated K-fold cross-validation was used to reduce over-fitting. Random Forest Classifier had the highest f1-score among the classifiers used. Hyperparameters of these classifiers were adjusted to increase the f1 score but still there was an issue of over-fitting. So thought of collection more data using a tool called TWINT. These tweets were approximately 15,000 in numbers and were manually labelled using a tool called Doccano. This is the step which is going on at present. Further work which remains is that the model needs to be trained on these labelled dataset and finally test the model on the previous dataset. Then the analysis of the tweets can be done further. This method can act as an effective way of recognizing the early sign of an user suffering from depression.

     
